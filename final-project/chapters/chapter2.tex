% chktex-file 3 chktex-file 9 chktex-file 17 chktex-file 18 chktex-file 36

\section{Spectral Theory in Finite Dimensional Spaces}

We're going to begin with the case where $X$ is a finite dimensional Banach space of complex numbers and $T: X \to X$ is an operator. According to the remark in the previous section for this chapter we are allowed to call $\sigma(T)$ the eigenvalues of $T$. For $\lambda \in \sigma(T)$ there exists multiple solutions $x \in X$, $x\neq 0$ for the equation $(\lambda I -T)x = 0$. These solutions are called the \textit{eigenvectors} and the set that contains all the solutions for the previous equation: $N_\lambda(T) := \ker(\lambda I - T)$ is called the \textit{geometric eigenspace} in $\lambda$. 

Also, for a positive integer $v$ define the set $N_\lambda^v := \ker ((\lambda I - T)^v)$ and note that if $n = \dim(X)$, then $N_\lambda^{v} \subseteq N_\lambda^{v+1}\subseteq N_\lambda^n = N_\lambda^{n+1}$. Let $v(\lambda) \leq n$ be a positive integer such that
\[ N_\lambda^{v(\lambda)-1} \subsetneq N_\lambda^{v(\lambda)} = N_\lambda^{v(\lambda) + 1} \]
and define the set $A_\lambda := N_\lambda^{v(\lambda)}$ which is called the \textit{algebraic eigenspace} in $\lambda$. Note that since $N_\lambda^0 = \ker(I) = \{0\}$ and $N_\lambda^1 \supsetneq \{0\}$ only when $\lambda\in \sigma(T)$, it follows that
\[ v(\lambda) > 0 \iff \lambda \in \sigma(T).  \]

The dimension of the eigenspaces in $\lambda$ are called \textit{geometric multiplicity} and \textit{algebraic multiplicity} of $\lambda$ respectively. The following examples will illustrate how this concepts work.

\begin{example} ($v(\lambda)$ and the algebraic multiplicity in $\lambda$ are not the same)

    \noindent Let $X = \C^3$ and define $T$ by its matrix representation:
    \[ T = \left[ \begin{matrix}
        0 & 0 & 1\\
        0 & 0 & 0\\
        0 & 0 & 0
    \end{matrix} \right].\]
    Note that $\sigma(T) = \{0\}$, $N_0 = \span\{(1,0,0)',(0,1,0)'\}$ and since $T^2 = 0$, $A_0 = N_0^2 = \span\{(1,0,0)',(0,1,0)', (0,0,1)\}$. Therefore, $v(0) = 2$ and the algebraic multiplicity in 0 is 3. 
\end{example}

\begin{example} (The geometric and algebraic multiplicities are not the same)

    \noindent Let $X = \C^3$ and define $T$ by its matrix representation:
    \[ T = \left[ \begin{matrix}
        i & 0 & 0\\
        0 & 0 & 1\\
        0 & 0 & 0
    \end{matrix} \right]. \]
    Note that $\sigma(T) = \{i,0\}$ with $N_i = \span\{(1,0,0)'\}$ and $N_0 = \span\{(0,1,0)'\}$ so the geometric multiplicity of both eigenvalues is 1. However, after taking $(\lambda I - T)^2$ we obtain
    \[ (iI - T)^2 = \left[ \begin{matrix}
        0 & 0 & 0\\
        0 & -1 & 1\\
        0 & 0 & -1
    \end{matrix} \right],\quad 
    (0I - T)^2 = \left[ \begin{matrix}
        -1 & 0 & 0\\
        0 & 0 & 0\\
        0 & 0 & 0
    \end{matrix} \right]\]
\end{example}
So $N_i^2 = N_i$ and $N_0^2 = \span\{(0,1,0)', (0,0,1)'\}$ implying that $v(i) = 1$ and $v(0) = 2$. 

This also shows that the algebraic multiplicity in $i$ is 1 while the algebraic multiplicity in $0$ is 2. In this case the algebraic multiplicities of the eigenvalues sum to the space's dimension and the direct sum of the algebraic eigenspaces gives us $X$.

In general, we can prove that $X$ is decomposed by the direct sum of the algebraic eigenspaces of any operator $T: X \to X$. But, before that, we want to show a important theorem that states that for any polynomial $P : \C \to \C$ and an operator $T$, the operator given by $P(T)$ is identically zero if and only if for every $\lambda \in \sigma(T)$, the multiplicity of $\lambda$ as a zero of $P$ coincides with the index $v(\lambda)$.

\begin{theorem}
    For a complex polynomial $P$ and an operator $T: X \to X$ the following conditions are equivalent:
    \begin{itemize}
        \item[(a)] $P(T) = 0$
        \item[(b)] $\lambda$ is a zero of $P$ with multiplicity $v(\lambda)$ for every $\lambda \in \sigma(T)$.
    \end{itemize}
\end{theorem}

\begin{proof}
    \textbf{Preliminaries:} Since we're assuming that $X$ is finitely dimensional, let $n = \dim X$ and let $\{x_1,\ldots, x_n\}$ be a basis of $X$. Then, for every $k = 1,\ldots, n$, the set $\{x_k, T x_k, \ldots, T^n x_k\}$ with $n+1$ elements is linearly dependent, so there exists a non-zero and non-constant polynomial $S_k$ such that $S_k(T)x_k = 0$. Define the operator $R = S_1 \cdot S_2 \cdots S_n$ which satisfies $R(T) x_k$ for every $k = 1,\ldots, n$, and thus, since every $x \in X$ can be written as a linear combination of basis elements $x = a_1 x_1 + \cdots + a_n x_n$. It follows that for any operator $T$ there always exists a non-zero polynomial $R:\C \to \C$ such that $R(T) x = 0$ for every $x \in X$.

    Let $\lambda_1, \ldots, \lambda_m \in \C$ be the zeros of the polynomial $R$ so we can factorize them as follows
    \[ R(z) = \beta \prod_{j = 1}^{m} (\lambda_j - z)^{m_j}, \]
    that way $R(T) = \beta \prod_{j = 1}^{m} (\lambda_j I - T )^{m_j}$ and it doesn't matter the order that we multiply these factors because after expanding we should get the same expression:
    \[ (aI+T)(bI+T) = abI + (a+b) T + T^2 = (bI+T)(aI+T). \]
    There are two possible scenarios for $\lambda_j$:
    \begin{itemize}
        \item If $\lambda_j \in \rho(T)$, then $(\lambda_j I - T)$ invertible. So after reordering the set of zeros of $R$ in such way $\{\lambda_1,\ldots,\lambda_p\} \subset \sigma(T)$ and $\{\lambda_{p+1},\ldots,\lambda_m\} \subset \rho(T)$ we obtain
        \[ R(T) = \beta \underbrace{\prod_{j = p+1}^{m} (\lambda_j I - T )^{m_j}}_{R_1(T)}\times \underbrace{\prod_{j = 1}^{p} (\lambda_j I - T )^{m_j} }_{R_2(T)}.  \]
        Now, note that for every $x\in X$, $R(T)(x) = R_1(T)R_2(T)(x) = 0$ and $R_1(T)$ is invertible because is the product of invertible operators. Therefore, since $R_1(T) y = 0$ if and only if $y = 0$, it follows that $R_2(T) = 0$.
        \item For $\lambda_j \in \sigma(T)$ and $x \in X$ if $(\lambda_j I - T )^{m_j} x = 0$, then $x \in A_\lambda(T)$ so it follows that $(\lambda_j I - T )^{v(\lambda_j)} x = 0$. Therefore, if we define
        \[ R_3(T) = \prod_{j = 1}^{p} (\lambda_j I - T )^{v(\lambda_j)}, \]
        then $R_3(T) = 0$. 
    \end{itemize}

    $\boldsymbol{(b) \implies (a)}$: If every $\lambda \in \sigma(T)$ is a zero of $P$ with multiplicity $v(\lambda)$, then $P$ is divisible by $R_3$ and thus, for some polynomial $Q$, $P(T) = R_3(T)Q(T) = 0$.

    $\boldsymbol{(a) \implies (b)}$: Now let $P(T) = 0$ and
    \[ P(z) = \alpha \prod_{j = 1}^{q} (\lambda_j - z)^{\alpha_j}.  \]
    Using the same argument as before, one can ignore the factors $(\lambda_j - z)$ if $\lambda_j \in \rho(T)$, so assume without restriction that $\{\lambda_1,\ldots, \lambda_q\} \subseteq \sigma(T)$. On the other hand, let $\lambda_0 \in \sigma(T)$ and note that there exists $y \neq 0$ for which $T y = \lambda_0 y$. Since
    \[  0 = P(T) y = P(\lambda_0) y,\; y\neq 0 \implies P(\lambda_0) = 0, \]
    it follows that $\lambda_0 \in \{\lambda_1,\ldots, \lambda_q\}$, and thus, $\sigma(T) \subseteq \{\lambda_1,\ldots, \lambda_q\}$. It is left to prove that $\alpha_j \geq v(\lambda_j)$ for every $j = 1,\ldots, q$, so for the sake of contradiction assume $\alpha_j < v(\lambda_j)$ for some $j = 1,\ldots q$. Assume without restriction that $j = 1$ and note that $N_{\lambda_1}^{\alpha_1} \subsetneq N_{\lambda_1}^{\alpha_1 + 1}$ so there exists $x_1 \in X$ such that
    \[ (\lambda_1 I - T)^{\alpha_1 + 1}x_1 = 0,\quad y_1 = (\lambda_1 I - T)^{\alpha_1}x_1 \neq 0.  \]
    Now, let $Q$ be a polynomial such that $P(z) = Q(z)(\lambda_1 - z)^{a_1} $ and $Q(\lambda_1) \neq 0$. Finally, since $T y_1 = \lambda_1 y_1$, it follows that
    \[ P(T)x_1 =  Q(T)(\lambda_1 I - T)^{\alpha_1}x_1 = Q(T) y_1 = \underbrace{Q(\lambda_1)}_{\neq 0} \cdot \underbrace{y_1}_{\neq 0} \neq 0, \]
    which leads to a contradiction with the fact that $P(T) = 0$. Therefore, every $\lambda\in \sigma(T)$ is a zero of multiplicity $v(\lambda)$ of the polynomial $P$.
\end{proof}

From this theorem there are multiple and important consequences. In the proof we also showed that there always exists a non-constant polynomial $R$ for which $R(T) = 0$. This polynomial, as the theorem implies must satisfy that each eigenvalue is a zero. Since non-constant polynomials have a finite number of zeros, but at least one, it follows that the number of eigenvalues in a finite dimensional space is finite and greater that 0.

\begin{corollary}
    For a finite dimensional space $X$, the spectrum of an operator is non-empty and finite.
\end{corollary}
$ $\hfill $\square$

Now, if we replace $P$ in the theorem with the difference between two polynomials we obtain the following corollary:

\begin{corollary}
    If $P, Q$ are polynomials, then $P(T) = Q(T)$ if for every $\lambda \in \sigma(T)$, $\lambda$ is a zero of multiplicity $v(\lambda)$ of the polynomial $P-Q$.
\end{corollary}
$ $\hfill $\square$

In fact, if for two polynomials $P,Q$, all their derivatives coincide at the spectrum of $T$, that is
\[ P^{(m)} (\lambda) = Q^{(m)}(\lambda),\quad \lambda \in \sigma(T),\; m < v(\lambda), \]
then they define the same operator $P(T) = Q(T)$. Furthermore, by generalizing this notion to a holomorphic function $f$, we have that $f(T)$ is characterized only by the values of $f$ and some of its derivatives at the spectrum of $T$.

Let $\F(T)$ be the family of all functions that are holomorphic at some open set containing $\sigma(T)$. For each function we can interpolate a polynomial $P$ such that
\[ f^{(m)} (\lambda) = P^{(m)}(\lambda),\quad \lambda \in \sigma(T),\; m < v(\lambda), \]
and with the previous corollary, we can be sure that there are no ambiguities if we define $f(T) = P(T)$ because any other polynomial that satisfies these equations would yield the same result. The following theorem immediately follows from the previous discussion.

\begin{theorem} If $f,g$ are functions in $\F(T)$ and $\alpha,\beta$ are complex numbers, then
    \begin{enumerate}[label = (\alph*)]
        \item $\alpha f + \beta g \in \F(T)$ and is defined as $(\alpha f + \beta g) (T) = \alpha f(T) + \beta g(T)$.
        \item $f \cdot g \in \F(T)$, is defined as $(f \cdot g) (T) = f(T)\cdot g(T)$, and also this implies $f(T)\cdot g(T) = g(T)\cdot f(T)$.
        \item If $f$ is a polynomial $f(z) = \sum_{n = 0}^{m} a_n z^n$, then $f(T) = \sum_{n = 0}^{m} a_n T^n$.
        \item $f(T) = 0$ if and only if
        \[ f^{(m)} (\lambda) = 0,\quad \lambda \in \sigma(T),\; m < v(\lambda). \]
    \end{enumerate}
\end{theorem}
$ $\hfill $\square$

Let $\lambda_0 \in \C$ and define $e_{\lambda_0}(z)$ to be a function that is equal to one at a neighborhood of $\lambda_0$ and zero at a neighborhood of each point in $\sigma(T) \backslash \{\lambda_0\}$ (the neighborhoods do not intersect). The function $e_{\lambda_0}$ is in fact holomorphic at an open set that contains $\sigma(T)$ although this set is not connected. 

Now define $E(\lambda_0) = e_{\lambda}(T)$, and apply the previous theorem to the following proposition

\begin{theorem} For the operator $E(\cdot)$ defined previously and $\lambda_0,\lambda_1 \in \C$:
    \begin{enumerate}[label = (\alph*)]
        \item $E(\lambda_0) = 0$ if and only if $\lambda_0 \in \rho(T)$.
        \item $E(\lambda_0)^2 = \E(\lambda_0) $.
        \item $E(\lambda_0)E(\lambda_1) = 0$ if $\lambda_0 \neq \lambda_1$.
        \item $I = \sum_{\lambda \in \sigma(T)} E(\lambda)$.
    \end{enumerate}
\end{theorem}

\begin{proof}Note that $e_{\lambda_0}$ is locally constant so all of its derivatives are zero, so according to the previous theorem, the only thing that matters is whether $e_{\lambda_0}(\lambda) = 0$ for every $\lambda \in \sigma(T)$ or not.
    \begin{enumerate}[label = (\alph*)]
        \item  If $\lambda \in \rho(T)$, then the neighborhood $V_0$ that contains $\lambda_0$ doesn't contain any eigenvalue of $T$, and thus, $e_{\lambda_0}(\lambda) = \1_{\lambda \in V_0} = 0$ for every $\lambda \in \sigma(T)$. If $E(\lambda) = 0$, then $e_{\lambda_0}(\lambda) = \1_{\lambda \in V_0} = 0$ for every $\lambda \in \sigma(T)$, and thus, all the eigenvalues are outside of $V_0$, implying that $\lambda_0$ is not an eigenvalue.
        \item The function $e_{\lambda_0}$ only takes two values $\{0,1\}$ and the squares of both values are equal to themselves. Thus, $e_{\lambda_0}^2(\lambda) = e_{\lambda_0}(\lambda)$ for every $\lambda_0, \lambda \in \C$ and $e_{\lambda_0}^2$ is also locally constant, so it follows that $e_{\lambda_0}^2(T) = e_{\lambda_0}(T)$.
        \item If $\lambda_0 \in \rho(T)$ or $\lambda_1 \in \rho(T)$, then $E(\lambda_0)E(\lambda_1) = 0$ by item (a), so assume without restriction that $\lambda_0,\lambda_1 \in \sigma(T)$. Note that $e_{\lambda_1}(\lambda_0) = e_{\lambda_0}(\lambda_1) = 0$ and $e_{\lambda_0}(\lambda) = e_{\lambda_1}(\lambda) = 0$ for any other eigenvalue of $T$ different from $\lambda_0$ and $\lambda_1$. Finally, $e_{\lambda_0}(z)\cdot e_{\lambda_1}(z)$ is identically 0 for any $z \in \sigma(T)$ and all the derivatives are 0, so it follows that, $e_{\lambda_0}(T)\cdot e_{\lambda_1}(T) = 0$.
        \item Let $V_1,\ldots, V_q$ be the disjoint neighborhoods for $\lambda_1, \ldots, \lambda_q$ for which $e_{\lambda_j}(\lambda) = \1_{\lambda \in V_j}$. Then, since all the sets are disjoint every eigenvalue is exactly in one of this sets, so $f(\lambda) := \sum_{j = 1}^{q}e_{\lambda_j}(\lambda) = 1$ for every $\lambda \in \sigma(T)$. Therefore, since $I = 1(T)$, and both $f$ and $1$ are locally constant functions, it follows that $\sum_{j = 1}^{q}e_{\lambda_j}(T) = I$.
    \end{enumerate}
\end{proof}

Now, let $\sigma(T) = \{\lambda_1,\ldots, \lambda_q\}$ and let $X_i = E(\lambda_i) X$. From item (b) of the previous theorem, it follows that $X_i\cap X_j = \{0\}$, from item (c) it follows that $E(\lambda_i) X_i = X_i$ and from item (d) it follows that $X = X_1+\cdots + X_q$. Therefore,

\begin{corollary}
    $X = X_1 \oplus \cdots \oplus X_q $.
\end{corollary}

On the other hand, note that for every $\lambda \neq \lambda_i \in \sigma(T)$, $\lambda$ is a zero of multiplicity $v(\lambda)$ of the function $e_{\lambda_i}$ and $\lambda_i$ is a zero of order $v(\lambda_i)$ of the function $z \mapsto (\lambda_i - z)^{v(\lambda_i)}$. Thus,
\[ (\lambda_i I - T)^{v(\lambda_i)} E(\lambda_i) = 0. \]

This relation shows that $(\lambda_i I - T)^{v(\lambda_i)} E(\lambda_i)X = (\lambda_i I - T)^{v(\lambda_i)} X_i = \{0\}$, and thus, since for every $y \in X_i$, $(\lambda_i I - T)^{v(\lambda_i)} y = 0$, it follows that $X_i \subseteq N_{\lambda_i}^{v(\lambda_i)} = A_{\lambda_i}$. The other inclusion is part of the following theorem

\begin{theorem}
    For $\lambda \in \sigma(T)$,
    \[ E(\lambda)X = N_\lambda^{v(\lambda)} = A_\lambda. \]
\end{theorem}

\begin{proof}
    In the previous paragraph we proved that $E(\lambda)X \subseteq A_{\lambda}$
\end{proof}